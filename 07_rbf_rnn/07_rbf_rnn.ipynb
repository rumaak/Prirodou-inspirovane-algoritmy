{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radial Basics Functions a Recurrent Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minule jsme si ukazovali a zkoušeli si naprogramovat jednoduché neuronové sítě. Dneska se podíváme na složitější struktury neuronových sítí Radial Basics Functions a rekurentní neuronové sítě.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial Basics Functions (RBF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naimplementujeme si nyní RBF síť. Implementace v tensorflow je jednoduchá -- stačí naimplementovat třídu, která bude mít dvě metody. Metoda *build* nainicializuje parametry podle velikosti vstupu a metoda *call* pak implementuje vlastní výpočet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katie\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFNetwork():\n",
    "    def __init__(self, input_dim, num_centers, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_centers = num_centers\n",
    "        self.centers = [np.random.uniform(-1, 1, input_dim) for i in range(num_centers)]\n",
    "        self.beta = 1\n",
    "        self.weights = None\n",
    "        \n",
    "    # spocteni hodnot neuronu na skryte vrstve\n",
    "    def calculate_activation(self, data):\n",
    "        hidden_layer_values = np.zeros((data.shape[0], self.num_centers), float)\n",
    "        for c_idx, c in enumerate(self.centers):\n",
    "            for x_idx, x in enumerate(data):\n",
    "                hidden_layer_values[x_idx,c_idx] = self.activation_fcn(c, x)\n",
    "        return hidden_layer_values\n",
    "    \n",
    "    #  hodnota aktivacni fce\n",
    "    def activation_fcn(self, centers, data):\n",
    "        return np.exp(-self.beta * np.linalg.norm(centers-data)**2)\n",
    "    \n",
    "    def fit(self, data, labels):\n",
    "        # zvol nahodne hodnoty pocatecnich centroidu\n",
    "        random_idx = np.random.permutation(data.shape[0])[:self.num_centers]\n",
    "        self.centers = [data[i,:] for i in random_idx]\n",
    "         \n",
    "        # spocitame aktivaci mezi vstupem a skrytou vrstvou\n",
    "        hidden_layer_values = self.calculate_activation(data)\n",
    "         \n",
    "        # porovname skutecne a predikovane vystupy a aktualizujem vahy \n",
    "        # pseudoinverzni matice je vlastne vzorecek pro LR pro train vah\n",
    "        self.weights = np.dot(np.linalg.pinv(hidden_layer_values), labels)\n",
    "          \n",
    "    def predict(self, data):\n",
    "        hidden_layer_values = self.calculate_activation(data)\n",
    "        labels = np.dot(hidden_layer_values, self.weights)\n",
    "        return labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zkusíme si naši RBF síť pustit na našem oblíbeném datasetu Iris. Načteme si data a labely, do které třídy data patří. Protože budeme dělat klasifikaci, je vhodné si labely převést na one-hot-encoding. Následně data rozdělíme na trénovací a testovací množinu, abychom mohli zvlášť data trénovat a zvlášť data testovat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "# ziskame data\n",
    "iris = datasets.load_iris()\n",
    "x, y = iris.data, iris.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n",
    "\n",
    "# delame klasifikaci prevedeme vystup do one hot encoding\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0]) \n",
    "y_train_onehot = onehotencoder.fit_transform(y_train.reshape(-1, 1)).toarray() \n",
    "y_test_onehot = onehotencoder.fit_transform(y_test.reshape(-1, 1)).toarray() \n",
    "\n",
    "# natrenujeme a ohodnotime sit\n",
    "rbf = RBFNetwork(4, 10, 3)\n",
    "rbf.fit(x_train, y_train_onehot)\n",
    "predicted = rbf.predict(x_test)\n",
    "y_pred = np.argmax(predicted, axis=1)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print('Accuracy: ' + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vidíme, že accuracy je v každém běhu dosti různá, což je způsobeno tím, že centroidy a betu zde nastavujeme náhodně, přestože jsme si říkali, že pozice centroidů se dá trénovat pomocí algoritmu k-means.\n",
    "\n",
    "### Úkol na cvičení\n",
    "\n",
    "Zkuste si naimplementovat algoritmus k-means pro inicializaci středů vstupních neuronů a zlepšit tím výstup sítě výše. Hint: metoda `add_variable` má parametr initializer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rekurentní neuronové sítě\n",
    "\n",
    "Rekurentní neuronová síť je síť, která navíc ke svému vstupu ještě bere jako další vstup svůj výstup z předchozího kroku. Proto výstupy z předchozí vrstvy mohou ovlivňovat vrstvu následující, což se může hodit například u generování časových řad nebo textu. Nejprve se podíváme, jak přesně by vypadala implementace jednoduché RNN, kdybychom si ji psali celou sami. \n",
    "\n",
    "Vytvoříme si nejprve jednoduchá v2ěty a budeme chtít, aby nám naše sít uměla predikovat, zda je daná věta pozitivní nebo negativní. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {\n",
    "  'good': True,\n",
    "  'bad': False,\n",
    "  'happy': True,\n",
    "  'sad': False,\n",
    "  'not good': False,\n",
    "  'not bad': True,\n",
    "  'not happy': False,\n",
    "  'not sad': True,\n",
    "  'very good': True,\n",
    "  'very bad': False,\n",
    "  'very happy': True,\n",
    "  'very sad': False,\n",
    "  'i am happy': True,\n",
    "  'this is good': True,\n",
    "  'i am bad': False,\n",
    "  'this is bad': False,\n",
    "  'i am sad': False,\n",
    "  'this is sad': False,\n",
    "  'i am not happy': False,\n",
    "  'this is not good': False,\n",
    "  'i am not bad': True,\n",
    "  'this is not sad': True,\n",
    "  'i am very happy': True,\n",
    "  'this is very good': True,\n",
    "  'i am very bad': False,\n",
    "  'this is very sad': False,\n",
    "  'this is very happy': True,\n",
    "  'i am good not bad': True,\n",
    "  'this is good not bad': True,\n",
    "  'i am bad not good': False,\n",
    "  'i am good and happy': True,\n",
    "  'this is not good and not happy': False,\n",
    "  'i am not at all good': False,\n",
    "  'i am not at all bad': True,\n",
    "  'i am not at all happy': False,\n",
    "  'this is not at all sad': True,\n",
    "  'this is not at all happy': False,\n",
    "  'i am good right now': True,\n",
    "  'i am bad right now': False,\n",
    "  'this is bad right now': False,\n",
    "  'i am sad right now': False,\n",
    "  'i was good earlier': True,\n",
    "  'i was happy earlier': True,\n",
    "  'i was bad earlier': False,\n",
    "  'i was sad earlier': False,\n",
    "  'i am very bad right now': False,\n",
    "  'this is very good right now': True,\n",
    "  'this is very sad right now': False,\n",
    "  'this was bad earlier': False,\n",
    "  'this was very good earlier': True,\n",
    "  'this was very bad earlier': False,\n",
    "  'this was very happy earlier': True,\n",
    "  'this was very sad earlier': False,\n",
    "  'i was good and not bad earlier': True,\n",
    "  'i was not good and not happy earlier': False,\n",
    "  'i am not at all bad or sad right now': True,\n",
    "  'i am not at all good or happy right now': False,\n",
    "  'this was not happy and not good earlier': False,\n",
    "}\n",
    "\n",
    "test_data = {\n",
    "  'this is happy': True,\n",
    "  'i am good': True,\n",
    "  'this is not happy': False,\n",
    "  'i am not good': False,\n",
    "  'this is not bad': True,\n",
    "  'i am not sad': True,\n",
    "  'i am very good': True,\n",
    "  'this is very bad': False,\n",
    "  'i am very sad': False,\n",
    "  'this is bad not good': False,\n",
    "  'this is good and happy': True,\n",
    "  'i am not good and not happy': False,\n",
    "  'i am not at all sad': True,\n",
    "  'this is not at all good': False,\n",
    "  'this is not at all bad': True,\n",
    "  'this is good right now': True,\n",
    "  'this is sad right now': False,\n",
    "  'this is very bad right now': False,\n",
    "  'this was good earlier': True,\n",
    "  'i was not happy and not good earlier': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nejprve je potřeba udělat nějaký preprocessing vět. Tím se myslí převést text do číselné reprezentace. To uděláme tak, že najdeme všechno unikátní slova, očíslujeme je, a každé slovo pak nahradíme jeho číslem. Následně uděláme one-hot-encoding každého slova (a to potom bude mít shape (vocab_size, 1)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'all', 'right', 'earlier', 'good', 'sad', 'happy', 'very', 'this', 'and', 'at', 'not', 'i', 'now', 'was', 'bad', 'or', 'is']\n",
      "Unique words: 18\n",
      "6\n",
      "am\n"
     ]
    }
   ],
   "source": [
    "# vytvorime si slovnik veskerych slov z dat\n",
    "vocab = list(set([w for text in train_data.keys() for w in text.split(' ')]))\n",
    "vocab_size = len(vocab)\n",
    "print(vocab)\n",
    "print('Unique words: ' + str(vocab_size))\n",
    "\n",
    "# kazde slovo nahradime jeho indexem\n",
    "word_to_idx = { w: i for i, w in enumerate(vocab) }\n",
    "idx_to_word = { i: w for i, w in enumerate(vocab) }\n",
    "print(word_to_idx['happy']) \n",
    "print(idx_to_word[0]) \n",
    "\n",
    "# prevedeme jeste slova do one hot reprezentace \n",
    "def create_inputs(text):\n",
    "    inputs = []\n",
    "    for w in text.split(' '):\n",
    "        v = np.zeros((vocab_size, 1))\n",
    "        v[word_to_idx[w]] = 1\n",
    "        inputs.append(v)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyní si napíšeme samotnou RNN. Informace z předchozího kroku se ukládá v RNN buňce do speciální proměnné -- tzv. skrytého stavu, který nám pak bude ovlivňovat další výstupy a bude se s každým novým vstupem v každém kroku aktualizovat. Aktuální krytý stav se počítá podle předchozího skrytého stavu a aktuálního vstupu. Výstup je spočítán pomocí aktuálního skrytého stavu. Pro každý krok se používají stejné 3 váhové matice: pro spoje z aktuálních vstupů do aktuálních skrytých, pro spoje z předchozích skrytých do aktuálních skrytých a pro spoje z aktuálních skrytých do vystupů. Zároveň potřebujeme bias pro spočtení skrytého stavu a další pro spočtení výstupu. Pomocí aktivační funkce tanh a dosazení hodnot do rovnice umíme vypočítat výstup a update skrytých stavů."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, input_size, output_size, hidden_size=64):\n",
    "        # vahy, deleni je kvuli snizeni variance \n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size)/1000\n",
    "        self.Wxh = np.random.randn(hidden_size, input_size)/1000\n",
    "        self.Why = np.random.randn(output_size, hidden_size)/1000\n",
    "\n",
    "        # biasy\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # zapamatovani si hodnoty predchoziho skryteho stavu\n",
    "        h = np.zeros((self.Whh.shape[0], 1))\n",
    "\n",
    "        # postupny update skrytého stavu\n",
    "        for i, x in enumerate(inputs):\n",
    "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
    "\n",
    "        # vypocet vystupniho vektoru\n",
    "        y = np.dot(self.Why, h) + self.by\n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49999536]\n",
      " [0.50000464]]\n"
     ]
    }
   ],
   "source": [
    "# definujeme si fci softmax, na namapovani hodnot do intervalu [0,1]  \n",
    "def softmax(x):\n",
    "    return np.exp(x) / sum(np.exp(x))\n",
    "\n",
    "# inicializace RNN\n",
    "inputs = create_inputs('i am very good')\n",
    "rnn = RNN(vocab_size, 2)\n",
    "y, h = rnn.forward(inputs)\n",
    "probs = softmax(y)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vidíme, že síť nám sice nějak funguje, ale není moc užitečná. Problém je, že nijak netrénujeme váhy. K tomu je potřeba si definovat ztrátovou funkci, použijeme cross-entropy loss, která se spočítá jako mínus logaritmus počtu správně predikovaných tříd.  Zároveň je potřeba dopsat zpětnou propagaci chyby, aby se síť mohla učit ze svých chyb a updatovat si váhy a skryté stavy. To je v podstatě jen derivace tanh, dosazení do vzorečků a použití řetízkového pravidla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, input_size, output_size, hidden_size=64):\n",
    "        # vahy, deleni je kvuli snizeni variance \n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size)/1000\n",
    "        self.Wxh = np.random.randn(hidden_size, input_size)/1000\n",
    "        self.Why = np.random.randn(output_size, hidden_size)/1000\n",
    "\n",
    "        # biasy\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # zapamatovani si hodnoty predchoziho skryteho stavu\n",
    "        h = np.zeros((self.Whh.shape[0], 1))\n",
    "\n",
    "        self.last_inputs = inputs\n",
    "        self.last_hs = { 0: h }\n",
    "\n",
    "        # postupny update skrytého stavu\n",
    "        for i, x in enumerate(inputs):\n",
    "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
    "            self.last_hs[i + 1] = h\n",
    "\n",
    "        # vypocet vystupniho vektoru\n",
    "        y = np.dot(self.Why, h) + self.by\n",
    "\n",
    "        return y, h\n",
    "    \n",
    "    def backprop(self, probs, target, learn_rate=2e-2):\n",
    "        # spocitame derivaci na vystupu podle chyby dL/dy\n",
    "        d_y = probs\n",
    "        d_y[target] -= 1\n",
    "        n = len(self.last_inputs)\n",
    "\n",
    "        # spocitame derivaci na vystupu z chyby pro vahy dL/dWhy a bias dL/dby \n",
    "        d_Why = np.dot(d_y, self.last_hs[n].T)\n",
    "        d_by = d_y\n",
    "\n",
    "        # inicializujeme si nulove dL/dWhh, dL/dWxh, a dL/dbh, budeme si do nich pocitat chybu \n",
    "        d_Whh = np.zeros(self.Whh.shape)\n",
    "        d_Wxh = np.zeros(self.Wxh.shape)\n",
    "        d_bh = np.zeros(self.bh.shape)\n",
    "\n",
    "        # spocteme dL/dh for pro posledni vystup h\n",
    "        d_h = np.dot(self.Why.T, d_y)\n",
    "\n",
    "        # backpropagujeme chybu zpet v case pomoci dosazovani do rovnic\n",
    "        for t in reversed(range(n)):\n",
    "            # pomocna hodnota: dL/dh * (1 - h^2)\n",
    "            temp = ((1 - self.last_hs[t + 1] ** 2) * d_h)\n",
    "\n",
    "            # dL/db = dL/dh * (1 - h^2)\n",
    "            d_bh += temp\n",
    "\n",
    "            # dL/dWhh = dL/dh * (1 - h^2) * h_{t-1}\n",
    "            d_Whh += np.dot(temp, self.last_hs[t].T)\n",
    "\n",
    "            # dL/dWxh = dL/dh * (1 - h^2) * x\n",
    "            d_Wxh += np.dot(temp, self.last_inputs[t].T)\n",
    "\n",
    "            # dL/dh = dL/dh * (1 - h^2) * Whh\n",
    "            d_h = np.dot(self.Whh, temp)\n",
    "\n",
    "        # abychom zabranili pirlis velkym gradientum, omezime hodnoty do intervalu [-1,1]\n",
    "        for d in [d_Wxh, d_Whh, d_Why, d_bh, d_by]:\n",
    "                  np.clip(d, -1, 1, out=d)\n",
    "\n",
    "        # update vah a biasu pomoci gradient descent\n",
    "        self.Whh -= learn_rate * d_Whh\n",
    "        self.Wxh -= learn_rate * d_Wxh\n",
    "        self.Why -= learn_rate * d_Why\n",
    "        self.bh -= learn_rate * d_bh\n",
    "        self.by -= learn_rate * d_by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 0\n",
      "Train:\tLoss 0.696 | Accuracy: 0.517\n",
      "Test:\tLoss 0.694 | Accuracy: 0.500\n",
      "--- Epoch 100\n",
      "Train:\tLoss 0.688 | Accuracy: 0.552\n",
      "Test:\tLoss 0.698 | Accuracy: 0.500\n",
      "--- Epoch 200\n",
      "Train:\tLoss 0.665 | Accuracy: 0.655\n",
      "Test:\tLoss 0.723 | Accuracy: 0.450\n",
      "--- Epoch 300\n",
      "Train:\tLoss 0.573 | Accuracy: 0.690\n",
      "Test:\tLoss 0.627 | Accuracy: 0.700\n",
      "--- Epoch 400\n",
      "Train:\tLoss 0.406 | Accuracy: 0.828\n",
      "Test:\tLoss 0.735 | Accuracy: 0.600\n",
      "--- Epoch 500\n",
      "Train:\tLoss 0.373 | Accuracy: 0.828\n",
      "Test:\tLoss 0.714 | Accuracy: 0.600\n",
      "--- Epoch 600\n",
      "Train:\tLoss 0.318 | Accuracy: 0.828\n",
      "Test:\tLoss 0.869 | Accuracy: 0.600\n",
      "--- Epoch 700\n",
      "Train:\tLoss 0.374 | Accuracy: 0.810\n",
      "Test:\tLoss 0.428 | Accuracy: 0.700\n",
      "--- Epoch 800\n",
      "Train:\tLoss 0.416 | Accuracy: 0.759\n",
      "Test:\tLoss 0.337 | Accuracy: 0.850\n",
      "--- Epoch 900\n",
      "Train:\tLoss 0.008 | Accuracy: 1.000\n",
      "Test:\tLoss 0.014 | Accuracy: 1.000\n",
      "--- Epoch 1000\n",
      "Train:\tLoss 0.003 | Accuracy: 1.000\n",
      "Test:\tLoss 0.005 | Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "def run_model(data, train):\n",
    "    items = list(data.items())\n",
    "    random.shuffle(items)\n",
    "\n",
    "    loss = 0\n",
    "    correct_answers = 0\n",
    "\n",
    "    for x, y in items:\n",
    "        inputs = create_inputs(x)\n",
    "        target = int(y)\n",
    "\n",
    "        # dopredny pruchod\n",
    "        out, _ = rnn.forward(inputs)\n",
    "        probs = softmax(out)\n",
    "\n",
    "        # spocitani loss/accuracy\n",
    "        loss -= np.log(probs[target])\n",
    "        correct_answers += int(np.argmax(probs) == target)\n",
    "\n",
    "        if train==True:\n",
    "            # pzetny pruchod\n",
    "            rnn.backprop(probs, target)\n",
    "\n",
    "    return loss/len(data), correct_answers/len(data)\n",
    "\n",
    "rnn = RNN(vocab_size, 2)\n",
    "\n",
    "for epoch in range(1001):\n",
    "    train_loss, train_accuracy = run_model(train_data, train=True)\n",
    "    if epoch % 100 == 0:\n",
    "        print('--- Epoch %d' % (epoch))\n",
    "        print('Train:\\tLoss %.3f | Accuracy: %.3f' % (train_loss, train_accuracy))\n",
    "\n",
    "        test_loss, test_accuracy = run_model(test_data, train=False)\n",
    "        print('Test:\\tLoss %.3f | Accuracy: %.3f' % (test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Sekvenční klasifikace pomocí LSTM\n",
    "\n",
    "Nyní, když chápeme, jak taková základní RNN funguje, zkusíme se podívat na složitější druh RNN -- LSTM sítě. Tyto sítě mají navíc uvnitř sebe paměťovou buňku a mechanizmus, který řídí,  jakou informaci si buňka pamatuje a jakou zapomíná. Zkusíme se na ni lépe podívat v následujícím příkladu sekvenční klasifikace. \n",
    "\n",
    "Sekvenční klasifikace je prediktivní modelovací problém, kdy máme na vstupu nějakou sekvenci v prostoru nebo čase a cílem je předpovědět kategorii této sekvence. Složitost tohoto problému spočívá v tom, že jednotvlivé sekvence mohou mít různou délku nebo mohou být složeny z rozsáhlého slovníku vstupních hodnot a mohou vužadovat, aby se model naučil nějaké dlouhodobé závislosti nebo kontext mezi vstupními sekvencemi.\n",
    "\n",
    "Zkusíme se tedy podívat na příklad sekvenční klasifikace pomocí LSTM na IMDB datasetu, což je dataset, který obsahuje slovní popis recenzí 50K filmů a následně klasifikaci, jestli byla recenze pozitivní nebo negativní v poměru 1:1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# zafixujem random seed pro reprodukovatelnost\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problém je, že slovní popis je nějak potřeba převést na číselnou reprezentaci. Naštěstí funkce ```imdb.load_data``` umí načíst data tak, že rovnou slova nahradí čísly a rozdělí je na train a test množiny v poměru 1:1. Navíc data načteme tak, že necháme jen prvních ```top_words``` nejčastějších slova  zbytek nahradíme 0. Dále je potřeba zkrátit nebo doplnit vstupní sekvence pro modelování tak, aby byly všechny stejně dlouhé, délku nastavíme na ```max_len```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 5000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "\n",
    "max_length = 500\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyní máme připravená data a můžeme si definovat a natrénovat model.\n",
    " - První vrstva je ```Embedding```, která používá vektory délky 32 pro každé slovo. \n",
    " - Další vrstva je ```LSTM``` vrstva, která obsahuje 100 paměťových jednotek (neuronů). \n",
    " - Na závěr použijeme ```Dense``` výstupní vrstvu s jedním neuronem a aktivační funkcí sigmoid k vytvoření predikcí 0 nebo 1, protože se jedná o klasifikační úlohu.\n",
    "\n",
    "Problém modelu je, že se velice snadno overfittuje na na daná trénovací data. Proto se se používají ještě vrstvy ```Dropout```, které spočívají v tom, že během trénování se náhodně vynechávají některé vstupy do další vrstvy. Tím se simuluje velký počet sítí s odlišnou strukturou a uzly jsou pak robustnější a omezuje se tím pádem overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 484s 19ms/step - loss: 0.4778 - acc: 0.7633\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 519s 21ms/step - loss: 0.2916 - acc: 0.8830\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 544s 22ms/step - loss: 0.3016 - acc: 0.8745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2580d55da20>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na závěr zkusíme predikovat výstupy na testovacích datech a podívat se, jak je model dobrý. Můžete si třeba zkusit pustit trénování modelu s droupoutem a bez něj a podívat se, jak se budou lišit výsledné accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.14%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generování textu znak po znaku \n",
    "\n",
    "Nyní se podíváme na jiný druh problému -- budeme generovat text znak po znaku, neboli natrénujeme jazykový model tak, že když mu pak dáme sekvenci znaků, tak nám model bude schopný předpovědět další znak. Jako trénovací množinu použijeme texty Nietzscheho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Example script to generate text from Nietzsche's writings.\n",
    "    At least 20 epochs are required before the generated text\n",
    "    starts sounding coherent.\n",
    "    It is recommended to run this script on GPU, as recurrent\n",
    "    networks are quite computationally intensive.\n",
    "    If you try this script on new data, make sure your corpus\n",
    "    has at least ~100k characters. ~1M is better.\n",
    "'''\n",
    "\n",
    "# nacteme si vstupni data\n",
    "path = tf.keras.utils.get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "text = open(path).read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = set(text)\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# rozdelime text na castecne zavisle sekvence znaku delky maxlen\n",
    "maxlen = 20\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "# prevedeme text na ciselne vektory\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "    \n",
    "# vytvorime si model    \n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars), activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# pomocna funkce k ziskani indexu z pravdepodobnostniho pole\n",
    "def sample(a, temperature=1.0):  \n",
    "    a = np.log(a) / temperature\n",
    "    a = np.exp(a) / np.sum(np.exp(a))\n",
    "    a = a/np.sum(a)\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))\n",
    "\n",
    "# natrenujeme model a po kazde iterace generujeme vystup\n",
    "for iteration in range(1, 60):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(X, y, batch_size=128, nb_epoch=1)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for _ in range(400):\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skript si samozřejmě můžete pustit, ale trénování poběží neskutečně dlouho. Proto jsme skript pustili na Google Colab a na výsledky se můžete podívat [zde](https://colab.research.google.com/drive/1B7zys275xmpPqahPwNvuYMPLmgvlV3l5) nebo [zde](/notebooks/07_rbf_rnn/results.txt)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
